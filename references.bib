
@misc{aicoffeebreakwithletitiaDiffusionModelsExplained2022,
  title = {Diffusion Models Explained. {{How}} Does {{OpenAI}}'s {{GLIDE}} Work?},
  author = {{AI Coffee Break with Letitia}},
  year = {2022},
  month = mar,
  abstract = {Diffusion models beat GANs in image synthesis, GLIDE generates images from text descriptions, surpassing even DALL-E in terms of photorealism! Check out this video to learn how diffusion models work. Enjoy the visuals! SPONSOR: Weights \& Biases üëâ https://wandb.me/ai-coffee-break ‚ùì Check out our daily \#MachineLearning Quiz Questions: https://www.youtube.com/c/AICoffeeBre...  Recommended videos: üì∫ DALL-E video: https://youtu.be/mvG2FGF0TvM üì∫ GAN explained video: https://youtu.be/\_qB4B6ttXk8 üì∫ CLIP video: https://youtu.be/dh8Rxhf7cLU Papers: üìú GLIDE paper: Nichol, Alex, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. "Glide: Towards photorealistic image generation and editing with text-guided diffusion models." arXiv preprint arXiv:2112.10741 (2021). https://arxiv.org/abs/2112.10741 üîó GLIDE mini, demo: https://huggingface.co/spaces/valhall...  üìú Diffusion models for image generation: Dhariwal, Prafulla, and Alexander Nichol. "Diffusion models beat GANs on image synthesis." Advances in Neural Information Processing Systems 34 (2021). https://arxiv.org/abs/2105.05233 üìú Original diffusion models paper: Sohl-Dickstein, Jascha, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. "Deep unsupervised learning using nonequilibrium thermodynamics." In International Conference on Machine Learning, pp. 2256-2265. PMLR, 2015. https://arxiv.org/abs/1503.03585 üîó Check out this awesome blogpost by Lilian Weng: https://lilianweng.github.io/lil-log/... üîó Flow-based models: https://lilianweng.github.io/lil-log/... üîó DALL-E blog post: https://openai.com/blog/dall-e/ Outline: 00:00 Diffusion models are cool 00:33 Weights \& Biases (Sponsor) 01:51 4 types of generative models (in 2022) 05:13 Diffusion models explained 08:27 Why are diffusion models good at photorealism? \textendash{} Diffusion models beat GANs 10:36 GLIDE explained 12:16 Classifier-guided diffusion, CLIP-guided diffusion 13:56 Classifier-free guidance Thanks to our Patrons who support us in Tier 2, 3, 4: üôè  Don Rosenthal, Dres. Trost GbR, banana.dev -- Kyle Morris, Joel Ang, Juli\'an Salazar, Edvard Gr\o dem {$\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf$} üî• Optionally, pay us a coffee to help with our Coffee Bean production!  {$\steaming$} Patreon: https://www.patreon.com/AICoffeeBreak Ko-fi: https://ko-fi.com/aicoffeebreak {$\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf$} ------------------------------------ üîó Links: AICoffeeBreakQuiz: https://www.youtube.com/c/AICoffeeBre... Twitter: https://twitter.com/AICoffeeBreak Reddit: https://www.reddit.com/r/AICoffeeBreak/ YouTube: https://www.youtube.com/AICoffeeBreak \#AICoffeeBreak \#MsCoffeeBean \#MachineLearning \#AI \#research\hspace{0pt} Video contains the rock emoji designed by OpenMoji \textendash{} the open-source emoji and icon project. License: CC BY-SA 4.0 Music üéµ :  Tell Me That I Can't (Instrumental) by NEFFEX}
}

@misc{aicoffeebreakwithletitiaImagenDALLECompetitor2022,
  title = {Imagen, the {{DALL-E}} 2 Competitor from {{Google Brain}}, Explained üß†| {{Diffusion}} Models Illustrated},
  author = {{AI Coffee Break with Letitia}},
  year = {2022},
  month = may,
  abstract = {Imagen from Google Brain üß† is competing with DALLE-2 when it comes to generating amazing images from just text! Here is an overview of Imagen, DALLE-2 and GLIDE, which are all diffusion-based text-to-image generators. SPONSOR: Weights \& Biases üëâ https://wandb.me/ai-coffee-break üì∫ Diffusion models and GLIDE explained: https://youtu.be/344w5h24-h8 üì∫ DALL-E: https://youtu.be/mvG2FGF0TvM üì∫ GPT-2 leaks training data: https://youtu.be/yhVlb-Hz6yM Thanks to our Patrons who support us in Tier 2, 3, 4: üôè  Don Rosenthal, Dres. Trost GbR, banana.dev -- Kyle Morris, Juli\'an Salazar, Edvard Gr\o dem, Vignesh Valliappan, Kevin Tsai, Mutual Information, Mike Ton Check out our daily \#MachineLearning Quiz Questions: https://www.youtube.com/c/AICoffeeBre... Paper üìú: Saharia, Chitwan, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho, David Fleet and Mohammad Norouzi. ``Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.'' (2022). https://arxiv.org/abs/2205.11487  üîó Imagen website (Google Brain): https://imagen.research.google/ üîó DrawBench: https://docs.google.com/spreadsheets/...  üîó MIT technology review: https://www.technologyreview.com/2022...  üí≠ Gary Marcus on Imagen compositionality: https://garymarcus.substack.com/p/hor... üñº Twitter thread generating images from your input text: https://twitter.com/mo\_norouzi/status... üìú VQGAN-CLIP paper: https://arxiv.org/abs/2204.08583 üìú High-Resolution Image Synthesis with Latent Diffusion Models paper: https://arxiv.org/abs/2112.10752 {$\RHD$} Outline: 00:00 Generating images from text 00:40 Weights\&Biases (Sponsor) 01:40 A brief history of text-to-image generators 04:44 How does Imagen work? 06:36 Classifier-free guidance 07:55 What is Imagen good at? \& DrawBench {$\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf$} üî• Optionally, pay us a coffee to help with our Coffee Bean production!  {$\steaming$} Patreon: https://www.patreon.com/AICoffeeBreak Ko-fi: https://ko-fi.com/aicoffeebreak {$\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf\blockuphalf$} üîó Links: AICoffeeBreakQuiz: https://www.youtube.com/c/AICoffeeBre... Twitter: https://twitter.com/AICoffeeBreak Reddit: https://www.reddit.com/r/AICoffeeBreak/ YouTube: https://www.youtube.com/AICoffeeBreak \#AICoffeeBreak \#MsCoffeeBean \#MachineLearning \#AI \#research\hspace{0pt} Music üéµ :  Til I Hear'em Say (Instrumental) - NEFFEX}
}

@misc{crowsonVQGANCLIPOpenDomain2022,
  title = {{{VQGAN-CLIP}}: {{Open Domain Image Generation}} and {{Editing}} with {{Natural Language Guidance}}},
  shorttitle = {{{VQGAN-CLIP}}},
  author = {Crowson, Katherine and Biderman, Stella and Kornis, Daniel and Stander, Dashiell and Hallahan, Eric and Castricato, Louis and Raff, Edward},
  year = {2022},
  month = apr,
  number = {arXiv:2204.08583},
  eprint = {2204.08583},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2204.08583},
  abstract = {Generating and editing images from open domain text prompts is a challenging task that heretofore has required expensive and specially trained models. We demonstrate a novel methodology for both tasks which is capable of producing images of high visual quality from text prompts of significant semantic complexity without any training by using a multimodal encoder to guide image generations. We demonstrate on a variety of tasks how using CLIP [37] to guide VQGAN [11] produces higher visual quality outputs than prior, less flexible approaches like DALL-E [38], GLIDE [33] and Open-Edit [24], despite not being trained for the tasks presented. Our code is available in a public repository.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/jason/repos/vault/zotero/storage/5GFFSZCP/Crowson et al. - 2022 - VQGAN-CLIP Open Domain Image Generation and Editi.pdf;/home/jason/repos/vault/zotero/storage/PYQG9P4K/2204.html}
}

@misc{cuencaServingDALLMini,
  title = {Serving {{DALL}}{$\cdot$}{{E Mini}}},
  author = {Cuenca, Pedro},
  abstract = {Our experience setting up a high-load backend infrastructure on Google TPUs.}
}

@misc{daymaDALLMini2021,
  title = {{{DALL}}{$\cdot$}{{E Mini}}},
  author = {Dayma, Boris and Patil, Suraj and Cuenca, Pedro and Saifullah, Khalid and Abraham, Tanishq and L{\^e} Kha{\u}\'c, Ph{\'u}c and Melas, Luke and Ghosh, Ritobrata},
  year = {2021},
  month = jul,
  doi = {10.5281/zenodo.5146400}
}

@misc{dhariwalDiffusionModelsBeat2021,
  title = {Diffusion {{Models Beat GANs}} on {{Image Synthesis}}},
  author = {Dhariwal, Prafulla and Nichol, Alex},
  year = {2021},
  month = jun,
  number = {arXiv:2105.05233},
  eprint = {2105.05233},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128\$\textbackslash times\$128, 4.59 on ImageNet 256\$\textbackslash times\$256, and 7.72 on ImageNet 512\$\textbackslash times\$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256\$\textbackslash times\$256 and 3.85 on ImageNet 512\$\textbackslash times\$512. We release our code at https://github.com/openai/guided-diffusion},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jason/repos/vault/zotero/storage/ZMEIKQYZ/Dhariwal and Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf;/home/jason/repos/vault/zotero/storage/6KXMYH76/2105.html}
}

@misc{dieleman2022diffusion,
  title = {Diffusion Models Are Autoencoders},
  author = {Dieleman, Sander},
  year = {2022}
}

@misc{dieleman2022guidance,
  title = {Guidance: A Cheat Code for Diffusion Models},
  author = {Dieleman, Sander},
  year = {2022}
}

@misc{dingCogView2FasterBetter2022,
  title = {{{CogView2}}: {{Faster}} and {{Better Text-to-Image Generation}} via {{Hierarchical Transformers}}},
  shorttitle = {{{CogView2}}},
  author = {Ding, Ming and Zheng, Wendi and Hong, Wenyi and Tang, Jie},
  year = {2022},
  month = may,
  number = {arXiv:2204.14217},
  eprint = {2204.14217},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.14217},
  abstract = {The development of the transformer-based text-to-image models are impeded by its slow generation and complexity for high-resolution images. In this work, we put forward a solution based on hierarchical transformers and local parallel auto-regressive generation. We pretrain a 6B-parameter transformer with a simple and flexible self-supervised task, Cross-modal general language model (CogLM), and finetune it for fast super-resolution. The new text-to-image system, CogView2, shows very competitive generation compared to concurrent state-of-the-art DALL-E-2, and naturally supports interactive text-guided editing on images.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/jason/repos/vault/zotero/storage/TPIMU926/Ding et al. - 2022 - CogView2 Faster and Better Text-to-Image Generati.pdf;/home/jason/repos/vault/zotero/storage/7RF8Z8AM/2204.html}
}

@misc{dingCogViewMasteringTexttoImage2021,
  title = {{{CogView}}: {{Mastering Text-to-Image Generation}} via {{Transformers}}},
  shorttitle = {{{CogView}}},
  author = {Ding, Ming and Yang, Zhuoyi and Hong, Wenyi and Zheng, Wendi and Zhou, Chang and Yin, Da and Lin, Junyang and Zou, Xu and Shao, Zhou and Yang, Hongxia and Tang, Jie},
  year = {2021},
  month = nov,
  number = {arXiv:2105.13290},
  eprint = {2105.13290},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2105.13290},
  abstract = {Text-to-Image generation in the general domain has long been an open problem, which requires both a powerful generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/jason/repos/vault/zotero/storage/EKV3DM5B/Ding et al. - 2021 - CogView Mastering Text-to-Image Generation via Tr.pdf;/home/jason/repos/vault/zotero/storage/XL9PCX8L/2105.html}
}

@misc{gafniMakeASceneSceneBasedTexttoImage2022,
  title = {Make-{{A-Scene}}: {{Scene-Based Text-to-Image Generation}} with {{Human Priors}}},
  shorttitle = {Make-{{A-Scene}}},
  author = {Gafni, Oran and Polyak, Adam and Ashual, Oron and Sheynin, Shelly and Parikh, Devi and Taigman, Yaniv},
  year = {2022},
  month = mar,
  number = {arXiv:2203.13131},
  eprint = {2203.13131},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {Recent text-to-image generation methods provide a simple yet exciting conversion capability between text and image domains. While these methods have incrementally improved the generated image fidelity and text relevancy, several pivotal gaps remain unanswered, limiting applicability and quality. We propose a novel text-to-image method that addresses these gaps by (i) enabling a simple control mechanism complementary to text in the form of a scene, (ii) introducing elements that substantially improve the tokenization process by employing domain-specific knowledge over key image regions (faces and salient objects), and (iii) adapting classifier-free guidance for the transformer use case. Our model achieves state-of-the-art FID and human evaluation results, unlocking the ability to generate high fidelity images in a resolution of 512x512 pixels, significantly improving visual quality. Through scene controllability, we introduce several new capabilities: (i) Scene editing, (ii) text editing with anchor scenes, (iii) overcoming out-of-distribution text prompts, and (iv) story illustration generation, as demonstrated in the story we wrote.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {/home/jason/repos/vault/zotero/storage/Q4SZ6IIP/Gafni et al. - 2022 - Make-A-Scene Scene-Based Text-to-Image Generation.pdf;/home/jason/repos/vault/zotero/storage/SEG9M6DZ/2203.html}
}

@misc{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  month = dec,
  number = {arXiv:2006.11239},
  eprint = {2006.11239},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jason/repos/vault/zotero/storage/B5WEJT9V/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf;/home/jason/repos/vault/zotero/storage/P5M82936/2006.html}
}

@article{mishkinDALLPreviewRisks2022,
  title = {{{DALL}}{$\cdot$}{{E}} 2 {{Preview}} - {{Risks}} and {{Limitations}}},
  author = {Mishkin, Pamela and Ahmad, Lama and Brundage, Miles and Krueger, Gretchen and Sastry, Girish},
  year = {2022}
}

@misc{nicholGLIDEPhotorealisticImage2022,
  title = {{{GLIDE}}: {{Towards Photorealistic Image Generation}} and {{Editing}} with {{Text-Guided Diffusion Models}}},
  shorttitle = {{{GLIDE}}},
  author = {Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark},
  year = {2022},
  month = mar,
  number = {arXiv:2112.10741},
  eprint = {2112.10741},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {/home/jason/repos/vault/zotero/storage/3QZBBSAC/Nichol et al. - 2022 - GLIDE Towards Photorealistic Image Generation and.pdf;/home/jason/repos/vault/zotero/storage/4FZDAYIN/2112.html}
}

@article{nicholImprovedDenoisingDiffusion2021,
  title = {Improved {{Denoising Diffusion Probabilistic Models}}},
  author = {Nichol, Alex and Dhariwal, Prafulla},
  year = {2021},
  month = feb,
  doi = {10.48550/arXiv.2102.09672},
  abstract = {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion},
  langid = {english},
  file = {/home/jason/repos/vault/zotero/storage/ZM3YIZ4N/Nichol and Dhariwal - 2021 - Improved Denoising Diffusion Probabilistic Models.pdf;/home/jason/repos/vault/zotero/storage/TZTEWX9M/2102.html}
}

@misc{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  month = feb,
  number = {arXiv:2103.00020},
  eprint = {2103.00020},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/jason/repos/vault/zotero/storage/Q5GIFLTI/Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf;/home/jason/repos/vault/zotero/storage/3ISPC8YC/2103.html}
}

@misc{rameshHierarchicalTextConditionalImage2022,
  title = {Hierarchical {{Text-Conditional Image Generation}} with {{CLIP Latents}}},
  author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  year = {2022},
  month = apr,
  number = {arXiv:2204.06125},
  eprint = {2204.06125},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2204.06125},
  abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/jason/repos/vault/zotero/storage/DQ6DLINR/Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation wit.pdf;/home/jason/repos/vault/zotero/storage/JI5L6C55/2204.html}
}

@misc{rameshHowDALLWorks,
  title = {How {{DALL}}{$\cdot$}{{E}} 2 {{Works}}},
  author = {Ramesh, Aditya},
  file = {/home/jason/repos/vault/zotero/storage/5Q2LBGYA/dalle2.html}
}

@misc{rameshZeroShotTexttoImageGeneration2021,
  title = {Zero-{{Shot Text-to-Image Generation}}},
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  year = {2021},
  month = feb,
  number = {arXiv:2102.12092},
  eprint = {2102.12092},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2102.12092},
  abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/jason/repos/vault/zotero/storage/HW5A2RD6/Ramesh et al. - 2021 - Zero-Shot Text-to-Image Generation.pdf;/home/jason/repos/vault/zotero/storage/P3XERW4Z/2102.html}
}

@misc{rombachHighResolutionImageSynthesis2022,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  year = {2022},
  month = apr,
  number = {arXiv:2112.10752},
  eprint = {2112.10752},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/jason/repos/vault/zotero/storage/R67F5IFF/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf;/home/jason/repos/vault/zotero/storage/ZANASF5B/2112.html}
}

@misc{sahariaPhotorealisticTexttoImageDiffusion2022,
  title = {Photorealistic {{Text-to-Image Diffusion Models}} with {{Deep Language Understanding}}},
  author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S. Sara and Lopes, Rapha Gontijo and Salimans, Tim and Ho, Jonathan and Fleet, David J. and Norouzi, Mohammad},
  year = {2022},
  month = may,
  number = {arXiv:2205.11487},
  eprint = {2205.11487},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2205.11487},
  abstract = {We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/jason/repos/vault/zotero/storage/ILFCVWDS/Saharia et al. - 2022 - Photorealistic Text-to-Image Diffusion Models with.pdf;/home/jason/repos/vault/zotero/storage/759IQ2QV/2205.html}
}

@misc{songScoreBasedGenerativeModeling2021,
  title = {Score-{{Based Generative Modeling}} through {{Stochastic Differential Equations}}},
  author = {Song, Yang and {Sohl-Dickstein}, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  year = {2021},
  month = feb,
  number = {arXiv:2011.13456},
  eprint = {2011.13456},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\textbackslash aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jason/repos/vault/zotero/storage/ZSUQAPK5/Song et al. - 2021 - Score-Based Generative Modeling through Stochastic.pdf;/home/jason/repos/vault/zotero/storage/U749TBX8/2011.html}
}

@misc{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/jason/repos/vault/zotero/storage/JFD5VER4/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/home/jason/repos/vault/zotero/storage/W8TSXM9U/1706.html}
}

@misc{wengWhatAreDiffusion2021,
  title = {What Are {{Diffusion Models}}?},
  author = {Weng, Lilian},
  year = {2021},
  month = jul,
  abstract = {[Updated on 2021-09-19: Highly recommend this blog post on score-based generative modeling by Yang Song (author of several key papers in the references)]. So far, I've written about three types of generative models, GAN, VAE, and Flow-based models. They have shown great success in generating high-quality samples, but each has some limitations of its own. GAN models are known for potentially unstable training and less diversity in generation due to their adversarial training nature.},
  chapter = {posts},
  howpublished = {https://lilianweng.github.io/posts/2021-07-11-diffusion-models/},
  langid = {english},
  file = {/home/jason/repos/vault/zotero/storage/V7IBRJBH/2021-07-11-diffusion-models.html}
}


