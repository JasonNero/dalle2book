
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>DALL·E 2 Presentation &#8212; DALL·E 2 Presentation</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Further Reading" href="bonus.html" />
    <link rel="prev" title="Introduction and Overview" href="intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/dalle2_fox_own.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">DALL·E 2 Presentation</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Introduction and Overview
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   DALL·E 2 Presentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bonus.html">
   Further Reading
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bibliography.html">
   Bibliography
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/JasonNero/dalle2book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/dalle2.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction-text-to-image">
   Introduction Text-To-Image
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#recent-work">
   Recent Work
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dalle">
     DALL·E
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#clip">
     CLIP
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#glide">
     GLIDE
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dalle-2">
   DALL·E 2
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#access">
     Access
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#diffusion-basics">
     Diffusion Basics
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#architecture">
     Architecture
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#prior">
       Prior
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#diffusion-prior">
         Diffusion Prior
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#decoder">
       Decoder
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#upsampler">
       Upsampler
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-dataset">
     Training Dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#limitations">
     Limitations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-work">
   Further Work
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#google-imagen">
     Google Imagen
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#questions-and-discussion">
   Questions and Discussion
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>DALL·E 2 Presentation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction-text-to-image">
   Introduction Text-To-Image
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#recent-work">
   Recent Work
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dalle">
     DALL·E
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#clip">
     CLIP
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#glide">
     GLIDE
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dalle-2">
   DALL·E 2
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#access">
     Access
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#diffusion-basics">
     Diffusion Basics
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#architecture">
     Architecture
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#prior">
       Prior
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#diffusion-prior">
         Diffusion Prior
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#decoder">
       Decoder
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#upsampler">
       Upsampler
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-dataset">
     Training Dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#limitations">
     Limitations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-work">
   Further Work
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#google-imagen">
     Google Imagen
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#questions-and-discussion">
   Questions and Discussion
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="dalle-2-presentation">
<h1><a class="toc-backref" href="#id49">DALL·E 2 Presentation</a><a class="headerlink" href="#dalle-2-presentation" title="Permalink to this headline">#</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Author: Jason Schuehlein (js450)<br />
Email: <a class="reference external" href="mailto:js450&#37;&#52;&#48;hdm-stuttgart&#46;de">js450<span>&#64;</span>hdm-stuttgart<span>&#46;</span>de</a></p>
</div>
<div class="contents topic" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#dalle-2-presentation" id="id49">DALL·E 2 Presentation</a></p>
<ul>
<li><p><a class="reference internal" href="#introduction-text-to-image" id="id50">Introduction Text-To-Image</a></p></li>
<li><p><a class="reference internal" href="#recent-work" id="id51">Recent Work</a></p>
<ul>
<li><p><a class="reference internal" href="#dalle" id="id52">DALL·E</a></p></li>
<li><p><a class="reference internal" href="#clip" id="id53">CLIP</a></p></li>
<li><p><a class="reference internal" href="#glide" id="id54">GLIDE</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#dalle-2" id="id55">DALL·E 2</a></p>
<ul>
<li><p><a class="reference internal" href="#access" id="id56">Access</a></p></li>
<li><p><a class="reference internal" href="#diffusion-basics" id="id57">Diffusion Basics</a></p></li>
<li><p><a class="reference internal" href="#architecture" id="id58">Architecture</a></p></li>
<li><p><a class="reference internal" href="#training-dataset" id="id59">Training Dataset</a></p></li>
<li><p><a class="reference internal" href="#limitations" id="id60">Limitations</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#further-work" id="id61">Further Work</a></p>
<ul>
<li><p><a class="reference internal" href="#google-imagen" id="id62">Google Imagen</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#questions-and-discussion" id="id63">Questions and Discussion</a></p></li>
</ul>
</li>
</ul>
</div>
<section id="introduction-text-to-image">
<h2><a class="toc-backref" href="#id50">Introduction Text-To-Image</a><a class="headerlink" href="#introduction-text-to-image" title="Permalink to this headline">#</a></h2>
<p><em>Text-To-Image (TTI)</em> ist ein Subtask der <em>Bildsynthese</em> und beschreibt <em>Conditional Image Generation</em>, also die Generierung von Bildsamples unter der Bedingung eines Labels <span class="math notranslate nohighlight">\(p(x|y)\)</span>. <em>Zero-Shot</em> TTI geht einen Schritt weiter und ermöglicht auch Generierung von Daten ausserhalb des gelernten Trainingsdatensatzes.</p>
<p><em>Generative Adversarial Nets (GAN)</em> sind häufig Vertreter im Bereich TTI.
Seit einiger Zeit werden aber auch verstärkt <em>Diffusion Models</em> verwendet, dazu später mehr.</p>
<p>Die Qualität von TTI Modellen wird gemessen an:</p>
<ul class="simple">
<li><p><strong>Fidelity</strong>: Übereinstimmung mit den Originalbildern</p></li>
<li><p><strong>Diversity</strong>: Abdeckung der gesamten Variation der Originalverteilung</p></li>
</ul>
<p>Kombiniert kann man das an der sog. <em>Fréchet inception distance (FID)</em> ablesen.</p>
<div class="dropdown admonition">
<p class="admonition-title">Fréchet inception distance (FID)</p>
<p>Die <em>Fréchet inception distance (FID)</em> ist eine Metrik zur Beurteilung der Qualität von Bildern generiert durch generative Modelle. Im Vergleich zum <em>Inception Score (IS)</em>, wird nicht nur die Verteilung der generierten Bilder betrachtet sondern die Verteilungen der echten sowie generierten Bilder verglichen. FID kombiniert <em>Fidelity</em> und <em>Diversity</em> in einer Zahl.</p>
<p>Die FID entspricht der quadrierten <em>Wasserstein Distanz</em> zwischen zwei multivariaten Gaussverteilungen.</p>
</div>
</section>
<section id="recent-work">
<h2><a class="toc-backref" href="#id51">Recent Work</a><a class="headerlink" href="#recent-work" title="Permalink to this headline">#</a></h2>
<div class="mermaid">
            gantt
    title Recent Publications around Text-To-Image
    dateFormat  YYYY-MM-DD
    axisFormat  %Y-%m
    todaymarker off
    section OpenAI
        DALL·E              :milestone, active, 2021-02-24,
        CLIP                :milestone, active, 2021-02-26,
        Guided Diffusion (Diffusion Models Beat GANs on Image Synthesis) :milestone, 2021-05-11,
        GLIDE               :milestone, active, 2021-12-20,
        DALL·E 2            :milestone, crit, 2022-04-13,
    section Other
        CogView             :milestone, 2021-05-26
        Lafite              :milestone, 2021-11-27,
        Latent Diffusion    :milestone, 2021-12-20,
        Make-A-Scene        :milestone, 2022-03-24,
        VQGAN-CLIP          :milestone, 2022-04-18,
        CogView 2           :milestone, 2022-04-28,
    section Google
        Imagen              :milestone, 2022-05-13,
        Parti               :milestone, 2022-06-22,
        </div><section id="dalle">
<h3><a class="toc-backref" href="#id52">DALL·E</a><a class="headerlink" href="#dalle" title="Permalink to this headline">#</a></h3>
<p>Der namentliche Vorgänger zu DALL·E 2 wurde am 5. Januar 2021 in einem <a class="reference external" href="https://openai.com/blog/dall-e/">OpenAI Blog Eintrag</a> vorgestellt und mehrere Wochen später, am 24. Februar 2021, wurde das Paper eingereicht mit dem Titel <a class="reference external" href="http://arxiv.org/abs/2102.12092">“Zero-Shot Text-to-Image Generation”</a> <span id="id1">[<a class="reference internal" href="bibliography.html#id26" title="Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-Shot Text-to-Image Generation. February 2021. URL: http://arxiv.org/abs/2102.12092 (visited on 2022-06-12), arXiv:2102.12092, doi:10.48550/arXiv.2102.12092.">RPG+21</a>]</span>.</p>
<p>DALL·E besteht aus zwei Modulen: dem <em>Discrete Variational Autoencoder (dVAE)</em> und einem <em>Decoder-Only Sparse Transformer</em>. Letzterer basiert nach eigenen Angaben auf einer Variante von GPT-3.</p>
<p>Lediglich der Code des dVAE Moduls wurde von OpenAI auf GitHub veröffentlicht unter <a class="reference external" href="https://github.com/openai/DALL-E"><code class="docutils literal notranslate"><span class="pre">openai/DALL-E</span></code></a>. Eine inoffizielle aber komplette Implementierung findet sich hier <a class="reference external" href="https://github.com/lucidrains/DALLE-pytorch"><code class="docutils literal notranslate"><span class="pre">lucidrains/DALLE-pytorch</span></code></a>.</p>
<p>Öffentlich zugänglich war das Model nie, alternativ kann man aber <a class="reference external" href="https://www.craiyon.com">CrAIyon</a> bzw. <a class="reference external" href="https://github.com/borisdayma/dalle-mini"><code class="docutils literal notranslate"><span class="pre">borisdayma/dalle-mini</span></code></a> verwenden, ein Versuch von <span id="id2">Dayma <em>et al.</em> [<a class="reference internal" href="bibliography.html#id10" title="Boris Dayma, Suraj Patil, Pedro Cuenca, Khalid Saifullah, Tanishq Abraham, Phúc Lê Kha\u ć, Luke Melas, and Ritobrata Ghosh. DALL·E Mini. July 2021. URL: https://github.com/borisdayma/dalle-mini, doi:10.5281/zenodo.5146400.">DPC+21</a>]</span>, DALL·E zu reproduzieren.</p>
<p>Folgende Eigenschaften von DALL·E sind herauszustellen:</p>
<ul class="simple">
<li><p><strong>Variable Binding</strong>: Korrektes Zuweisen von Attributen zu Objekten (<a class="reference internal" href="#dalle1-var-fig"><span class="std std-numref">Fig. 1</span></a>)</p></li>
<li><p>Mehrere Objekte in einem Bild (<a class="reference internal" href="#dalle1-var-fig"><span class="std std-numref">Fig. 1</span></a>)</p></li>
<li><p>Beachtung von Perspektive (<a class="reference internal" href="#dalle1-persp-fig"><span class="std std-numref">Fig. 2</span></a>)</p></li>
<li><p>Interne und Externe Objektstrukturen (<a class="reference internal" href="#dalle1-struct-fig"><span class="std std-numref">Fig. 3</span></a>)</p></li>
<li><p>Zeitliches und geographisches Wissen</p></li>
</ul>
<figure class="align-default" id="dalle1-var-fig">
<img alt="_images/dalle1_variable_binding.png" src="_images/dalle1_variable_binding.png" />
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">DALL·E - Multiple Objects and Variable Binding Examples.<br />
<a class="reference external" href="https://openai.com/blog/dall-e/">https://openai.com/blog/dall-e/</a>.</span><a class="headerlink" href="#dalle1-var-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-g-2 sd-g-xs-2 sd-g-sm-2 sd-g-md-2 sd-g-lg-2 docutils">
<div class="sd-col sd-d-flex-column docutils">
<figure class="align-default" id="dalle1-persp-fig">
<img alt="_images/dalle1_perspective.png" src="_images/dalle1_perspective.png" />
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">DALL·E - Perspective Examples.<br />
<a class="reference external" href="https://openai.com/blog/dall-e/">https://openai.com/blog/dall-e/</a>.</span><a class="headerlink" href="#dalle1-persp-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</div>
<div class="sd-col sd-d-flex-column docutils">
<figure class="align-default" id="dalle1-struct-fig">
<img alt="_images/dalle1_structures.png" src="_images/dalle1_structures.png" />
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">DALL·E - Perspective Examples.<br />
<a class="reference external" href="https://openai.com/blog/dall-e/">https://openai.com/blog/dall-e/</a>.</span><a class="headerlink" href="#dalle1-struct-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="clip">
<span id="id3"></span><h3><a class="toc-backref" href="#id53">CLIP</a><a class="headerlink" href="#clip" title="Permalink to this headline">#</a></h3>
<p>OpenAI’s <em>Contrastive Language Image Pretraining (CLIP)</em> von wurde am 26. Februar 2021 vorgestellt in <a class="reference external" href="http://arxiv.org/abs/2103.00020">“Learning Transferable Visual Models From Natural Language Supervision”</a> von <span id="id4">Radford <em>et al.</em> [<a class="reference internal" href="bibliography.html#id23" title="Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. February 2021. URL: http://arxiv.org/abs/2103.00020 (visited on 2022-06-07), arXiv:2103.00020.">RKH+21</a>]</span>.</p>
<p>CLIP besteht aus zwei Encodern, einem der Text in Text Embeddings umwandelt und ein weiterer der Bilder in Image Embeddings umwandelt.</p>
<figure class="align-default" id="id39">
<img alt="_images/clip-overview-a.svg" src="_images/clip-overview-a.svg" /><figcaption>
<p><span class="caption-number">Fig. 4 </span><span class="caption-text">Illustration of the CLIP Training.<br />
<span id="id5">[<a class="reference internal" href="bibliography.html#id23" title="Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. February 2021. URL: http://arxiv.org/abs/2103.00020 (visited on 2022-06-07), arXiv:2103.00020.">RKH+21</a>]</span>.</span><a class="headerlink" href="#id39" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Für einen Batch aus <span class="math notranslate nohighlight">\(N\)</span> Image-Text Paaren <span class="math notranslate nohighlight">\((x,y)\)</span> werden alle Text Embeddings allen Image Embeddings gegenübergestellt. CLIP wird nun darauf trainiert die <span class="math notranslate nohighlight">\(N\)</span> der <span class="math notranslate nohighlight">\(N \times N\)</span> möglichen Image-Text Paare zu identifizieren die korrekt sind. Verwendet wird hierbei die <em>Cosine Similarity</em>, diese soll für die korrekten Paare maximiert und die inkorrekten Paare minimiert werden.</p>
<p>Effektiv lernt CLIP ein <em>“joint representation space”</em> für Texte und Bilder.</p>
<div class="dropdown admonition">
<p class="admonition-title">Refresher: Cosine Similarity</p>
<p>Das Skalarprodukt</p>
<div class="math notranslate nohighlight">
\[\mathbf{a} \cdot \mathbf{b} = \| \mathbf{a} \|_2 \| \mathbf{b} \|_2 \cos{\theta}\]</div>
<p>wird umgeformt um die Cosine Similarity <span class="math notranslate nohighlight">\(S_C\)</span> zu erhalten</p>
<div class="math notranslate nohighlight">
\[S_C(\mathbf{a}, \mathbf{b}) := \cos{\theta} = \frac{\mathbf{a} \cdot \mathbf{b}}{\| \mathbf{a} \|_2 \| \mathbf{b} \|_2}\]</div>
</div>
</section>
<section id="glide">
<span id="id6"></span><h3><a class="toc-backref" href="#id54">GLIDE</a><a class="headerlink" href="#glide" title="Permalink to this headline">#</a></h3>
<figure class="align-default" id="id40">
<img alt="_images/glide_header.png" src="_images/glide_header.png" />
<figcaption>
<p><span class="caption-number">Fig. 5 </span><span class="caption-text">Cherrypicked generations of GLIDE <span id="id7">[<a class="reference internal" href="bibliography.html#id21" title="Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. March 2022. URL: http://arxiv.org/abs/2112.10741 (visited on 2022-06-07), arXiv:2112.10741.">NDR+22</a>]</span>.</span><a class="headerlink" href="#id40" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>GLIDE ist ein weiteres TTI Model von OpenAI und wurde am 20. Dezember 2021 in dem gleichnamigen Paper <a class="reference external" href="http://arxiv.org/abs/2112.10741">“GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models”</a> von <span id="id8">Nichol <em>et al.</em> [<a class="reference internal" href="bibliography.html#id21" title="Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. March 2022. URL: http://arxiv.org/abs/2112.10741 (visited on 2022-06-07), arXiv:2112.10741.">NDR+22</a>]</span> vorgestellt.</p>
<p>Zwar bekam GLIDE keinen eigenen Blog Eintrag auf der OpenAI Website, dafür wurde aber der Code offiziell auf GitHub veröffentlicht unter <a class="reference external" href="https://github.com/openai/glide-text2im"><code class="docutils literal notranslate"><span class="pre">openai/glide-text2im</span></code></a>. Neben Checkpoints für eine kleinere Version, genannt <em>“GLIDE (filtered)”</em>, gibt es dort auch Notebooks zur Inferenz von Text-To-Image und Inpainting.</p>
<p>Trainiert wurde diese kleine Version mit stark gefilterten Trainingsdaten, um Gewalt und Hass Symbole zu entfernen. Sogar Humanoiden wurden komplett entfernt. Durch die relativ kleine Grösse scheitert GLIDE (filtered) jedoch häufig an Variable Binding (siehe <a class="reference internal" href="#glide-filtered-fig"><span class="std std-numref">Fig. 6</span></a>)</p>
<figure class="align-default" id="glide-filtered-fig">
<img alt="_images/glide_filtered_comparison.png" src="_images/glide_filtered_comparison.png" />
<figcaption>
<p><span class="caption-number">Fig. 6 </span><span class="caption-text">Shortcomings of GLIDE (filtered) <span id="id9">[<a class="reference internal" href="bibliography.html#id21" title="Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. March 2022. URL: http://arxiv.org/abs/2112.10741 (visited on 2022-06-07), arXiv:2112.10741.">NDR+22</a>]</span></span><a class="headerlink" href="#glide-filtered-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="dalle-2">
<h2><a class="toc-backref" href="#id55">DALL·E 2</a><a class="headerlink" href="#dalle-2" title="Permalink to this headline">#</a></h2>
<figure class="align-default" id="id41">
<img alt="_images/dalle2_header.png" src="_images/dalle2_header.png" />
<figcaption>
<p><span class="caption-number">Fig. 7 </span><span class="caption-text">Cherrypicked generations of DALL·E 2 <span id="id10">[<a class="reference internal" href="bibliography.html#id24" title="Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical Text-Conditional Image Generation with CLIP Latents. April 2022. URL: http://arxiv.org/abs/2204.06125 (visited on 2022-06-09), arXiv:2204.06125, doi:10.48550/arXiv.2204.06125.">RDN+22</a>]</span>.</span><a class="headerlink" href="#id41" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<!-- 
```{figure} attachments/dalle2explained_polarbear.gif
:height: 256px

DALL·E 2 creating an image of a polarbear playing bass.  
https://openai.com/dall-e-2/.
``` 
-->
<p><strong>DALL·E 2</strong> wurde von OpenAI entwickelt und mit dem <a class="reference external" href="https://openai.com/dall-e-2/">Blog Post</a> des Projekts am <em>06. April 2022</em> veroeffentlicht. Das dazugehörige Paper <em><a class="reference external" href="https://arxiv.org/abs/2204.06125">Hierarchical Text-Conditional Image Generation with CLIP Latents</a></em> von <span id="id11">Ramesh <em>et al.</em> [<a class="reference internal" href="bibliography.html#id24" title="Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical Text-Conditional Image Generation with CLIP Latents. April 2022. URL: http://arxiv.org/abs/2204.06125 (visited on 2022-06-09), arXiv:2204.06125, doi:10.48550/arXiv.2204.06125.">RDN+22</a>]</span> wurde eine Woche später am <em>13. April 2022</em> vorgelegt und beschreibt <strong>unCLIP</strong>, die grundlegende Architektur hinter DALL·E 2. Das Deployment dessen, die sog. <strong>DALL·E 2 Preview</strong> ist eine modifizierte <em>“production version”</em> <span id="id12">[<a class="reference internal" href="bibliography.html#id24" title="Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical Text-Conditional Image Generation with CLIP Latents. April 2022. URL: http://arxiv.org/abs/2204.06125 (visited on 2022-06-09), arXiv:2204.06125, doi:10.48550/arXiv.2204.06125.">RDN+22</a>]</span>.</p>
<p>Bis zur Veröffentlichung von Googles Imagen war DALL·E 2 State of the Art im Bereich Text-To-Image.</p>
<p>Im Vergleich zu seinem namentlichen Vorgänger hat DALL·E 2 eine höhere Auflösung und ein höheres Level an Photorealismus (siehe <a class="reference internal" href="#dalle1-fox-fig"><span class="std std-numref">Fig. 9</span></a> vs. <a class="reference internal" href="#dalle2-fox-fig"><span class="std std-numref">Fig. 11</span></a>).</p>
<div class="dropdown admonition">
<p class="admonition-title">Claude Monet</p>
<figure class="align-default" id="id42">
<img alt="_images/monet_wheat_field.jpg" src="_images/monet_wheat_field.jpg" />
<figcaption>
<p><span class="caption-number">Fig. 8 </span><span class="caption-text">The Wheat Field, Claude Monet (1881)</span><a class="headerlink" href="#id42" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</div>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-g-2 sd-g-xs-2 sd-g-sm-2 sd-g-md-2 sd-g-lg-2 docutils">
<div class="sd-col sd-d-flex-column docutils">
<figure class="align-default" id="dalle1-fox-fig">
<img alt="_images/dalle1_fox.jpg" src="_images/dalle1_fox.jpg" />
<figcaption>
<p><span class="caption-number">Fig. 9 </span><span class="caption-text">DALL·E 1: “a painting of a fox sitting in a field at sunrise in the style of Claude Monet”
<a class="reference external" href="https://openai.com/dall-e-2/">https://openai.com/dall-e-2/</a>.</span><a class="headerlink" href="#dalle1-fox-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</div>
<div class="sd-col sd-d-flex-column docutils">
<figure class="align-default" id="glide-fox-fig">
<img alt="_images/glide_fox.png" src="_images/glide_fox.png" />
<figcaption>
<p><span class="caption-number">Fig. 10 </span><span class="caption-text">GLIDE: “a painting of a fox in the style of starry night” <span id="id13">[<a class="reference internal" href="bibliography.html#id21" title="Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. March 2022. URL: http://arxiv.org/abs/2112.10741 (visited on 2022-06-07), arXiv:2112.10741.">NDR+22</a>]</span></span><a class="headerlink" href="#glide-fox-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</div>
<div class="sd-col sd-d-flex-column docutils">
<figure class="align-default" id="dalle2-fox-fig">
<img alt="_images/dalle2_fox.jpg" src="_images/dalle2_fox.jpg" />
<figcaption>
<p><span class="caption-number">Fig. 11 </span><span class="caption-text">DALL·E 2: “a painting of a fox sitting in a field at sunrise in the style of Claude Monet”
<a class="reference external" href="https://openai.com/dall-e-2/">https://openai.com/dall-e-2/</a>.</span><a class="headerlink" href="#dalle2-fox-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
</div>
<p>Neben <em>Text-to-Image</em> besitzt DALL·E 2 aber auch noch weitere Fähigkeiten:</p>
<ul class="simple">
<li><p>Editieren von Bildern durch <strong>Inpainting</strong> (siehe <a class="reference internal" href="#inpainting2-fig"><span class="std std-numref">Fig. 12</span></a>),</p></li>
<li><p><strong>Varianten</strong> eines Input Bildes erzeugen (siehe <a class="reference internal" href="#variations-fig"><span class="std std-numref">Fig. 13</span></a>) und</p></li>
<li><p><strong>Text Diffs</strong> - Durch Text gesteuerte Manipulation im Latent Space (siehe <a class="reference internal" href="#textdiff-fig"><span class="std std-numref">Fig. 14</span></a>).</p></li>
</ul>
<figure class="align-default" id="inpainting2-fig">
<img alt="_images/dalle2explained_mona_inpainting.gif" src="_images/dalle2explained_mona_inpainting.gif" />
<figcaption>
<p><span class="caption-number">Fig. 12 </span><span class="caption-text">DALL·E 2 giving Mona Lisa a mohawk via Inpainting.<br />
<a class="reference external" href="https://openai.com/dall-e-2/">https://openai.com/dall-e-2/</a>.</span><a class="headerlink" href="#inpainting2-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="variations-fig">
<img alt="_images/dalle2_fox_variants.png" src="_images/dalle2_fox_variants.png" />
<figcaption>
<p><span class="caption-number">Fig. 13 </span><span class="caption-text">DALL·E 2 Variations on “a painting of a fox sitting in a field at sunrise in the style of Claude Monet”. Created by myself with DALL·E 2: <a class="reference external" href="https://labs.openai.com/s/fb5LwnfumVDJ5NbNFiql16XJ">https://labs.openai.com/s/fb5LwnfumVDJ5NbNFiql16XJ</a></span><a class="headerlink" href="#variations-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="textdiff-fig">
<img alt="_images/textdiff_house.gif" src="_images/textdiff_house.gif" />
<figcaption>
<p><span class="caption-number">Fig. 14 </span><span class="caption-text">DALL·E 2 Text Diff <span id="id14">[<a class="reference internal" href="bibliography.html#id25" title="Aditya Ramesh. How DALL·E 2 Works. URL: http://adityaramesh.com/posts/dalle2/dalle2.html (visited on 2022-06-21).">Ram</a>]</span>: <span class="math notranslate nohighlight">\((\text{image of victorian house}) + \text{&quot;a modern house&quot;} − \text{&quot;a victorian house&quot;}\)</span></span><a class="headerlink" href="#textdiff-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Initial bekamen 400 ausgewählte Personen Zugriff auf eine API über die Inferenzen durchgeführt werden können und über eine Waitlist werden weitere Nutzer zugelassen. Stand 18. Mai 2022 wurden bereits 3 Millionen Bilder generiert und es sollen <span class="math notranslate nohighlight">\(\approx 1000\)</span> neue Nutzer pro Woche freigeschaltet werden <span id="id15">[<a class="reference internal" href="bibliography.html#id9" title="DALL·E 2 Research Preview Update. May 2022. URL: https://openai.com/blog/dall-e-2-update/ (visited on 2022-06-25).">DAL22</a>]</span>.</p>
<p>Offizieller Grund für den beschränkten Zugang sind Sicherheitsbedenken seitens OpenAI <span id="id16">[<a class="reference internal" href="bibliography.html#id20" title="Pamela Mishkin, Lama Ahmad, Miles Brundage, Gretchen Krueger, and Girish Sastry. DALL·E 2 Preview - Risks and Limitations. 2022. URL: https://github.com/openai/dalle-2-preview/blob/main/system-card.md.">MAB+22</a>]</span>.</p>
<figure class="align-default" id="dalle2-architecture-fig">
<img alt="_images/dalle2_figure2.png" src="_images/dalle2_figure2.png" />
<figcaption>
<p><span class="caption-number">Fig. 15 </span><span class="caption-text">Overview of DALL·E 2 <span id="id17">[<a class="reference internal" href="bibliography.html#id24" title="Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical Text-Conditional Image Generation with CLIP Latents. April 2022. URL: http://arxiv.org/abs/2204.06125 (visited on 2022-06-09), arXiv:2204.06125, doi:10.48550/arXiv.2204.06125.">RDN+22</a>]</span>.</span><a class="headerlink" href="#dalle2-architecture-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>In <a class="reference internal" href="#dalle2-architecture-fig"><span class="std std-numref">Fig. 15</span></a> ist die grundlegende Architektur dargestellt, sie besteht aus 3 Elementen:</p>
<ul class="simple">
<li><p><strong>CLIP Model</strong> um Text Embeddings zu generieren</p></li>
<li><p><strong>Prior</strong> um Text Embeddings in Image Embeddings umzuwandeln</p></li>
<li><p><strong>Decoder</strong> um aus Image Embeddings ein Bild zu generieren</p></li>
</ul>
<p>Tatsächlich ist DALL·E 2 eher eine Weiterentwicklung von GLIDE (<a class="reference internal" href="#glide-fox-fig"><span class="std std-numref">Fig. 10</span></a>). Die eigentliche Neuerung besteht darin, dass ein Diffusion Model statt auf Text Encodings, nun auf CLIP Image Embeddings trainiert wird und diese rückgängig macht, daher auch der Name <em>“unCLIP”</em>. Aus diesem “Umweg” resultieren vorteilhafte Eigenschaften, so kann man beispielsweise den CLIP Latent Space erkunden und visualisieren.</p>
<p>Ähnlich wie bei DALL·E, wurde kein Code des Models veröffentlicht, deshalb wurde von der Open-Source Community ein Versuch gestartet, DALL·E 2 (bzw. das im Paper spezifizierte unCLIP) zu reproduzieren, es gibt also eine inoffizielle Implementierung in dem GitHub Repository <a class="reference external" href="https://github.com/lucidrains/DALLE2-pytorch"><code class="docutils literal notranslate"><span class="pre">lucidrains/DALLE2-pytorch</span></code></a>.
Diese ist grösstenteils abgeschlossen, und Stand 25. Juni 2022 trainiert die Community des AI Vereins <a class="reference external" href="https://laion.ai/#top">LAION</a> einen ersten <a class="reference external" href="https://huggingface.co/zenglishuci/conditioned-prior">Prior</a> und auch erste <a class="reference external" href="https://huggingface.co/Veldrovive/DA-VINC-E">Decoder</a> sind in der Community zu finden.</p>
<p>Ein erstes Inferenz Notebook der Community welches findet sich unter <a class="reference external" href="https://github.com/LAION-AI/dalle2-laion"><code class="docutils literal notranslate"><span class="pre">LAION-AI/dalle2-laion</span></code></a>.</p>
<figure class="align-default" id="id43">
<img alt="_images/laion_dalle2_fox.png" src="_images/laion_dalle2_fox.png" />
<figcaption>
<p><span class="caption-number">Fig. 16 </span><span class="caption-text">Inference on the same fox prompt as above, using <a class="reference external" href="https://github.com/LAION-AI/dalle2-laion"><code class="docutils literal notranslate"><span class="pre">LAION-AI/dalle2-laion</span></code></a>. Inference Time: ~10min (Tesla T4).</span><a class="headerlink" href="#id43" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference external" href="https://thisimagedoesnotexist.com">ThisImageDoesNotExist</a> ist eine kleine Demo bei der man raten muss welche Bilder von einem Menschen sind und welche von DALL·E 2 generiert wurden. Der Durchschnitt liegt bei <span class="math notranslate nohighlight">\(18/30\)</span> korrekt zugeordneten Bildern.</p>
<section id="access">
<h3><a class="toc-backref" href="#id56">Access</a><a class="headerlink" href="#access" title="Permalink to this headline">#</a></h3>
<p>Unter den 400 Auserwählten waren zu Beginn nur 200 OpenAI Mitarbeiter, 10 Künstler, “ein paar Dutzend” anerkannte Wissenschaftler und 165 “company friends”. Über eine Waitlist wurden über Zeit auch weiteren Personen eingeladen, bis zu 1.000 Personen pro Woche <span id="id18">[<a class="reference internal" href="bibliography.html#id9" title="DALL·E 2 Research Preview Update. May 2022. URL: https://openai.com/blog/dall-e-2-update/ (visited on 2022-06-25).">DAL22</a>]</span>.</p>
<p>Die Verwendung der API ist nur für persönliche, nicht-kommerzielle oder wissenschaftliche Zwecke zulässig. User müssen beim Posten von generierten Bildern eindeutig kennzeichnen ob und welcher Teil des Bildes von DALL·E 2 generiert wurde. Ausserdem enthält jedes generierte Bild ein kleines Wasserzeichen in der unteren rechten Bildecke (siehe <a class="reference internal" href="#signature-fig"><span class="std std-numref">Fig. 17</span></a>).</p>
<figure class="align-default" id="signature-fig">
<img alt="_images/signature_closeup.png" src="_images/signature_closeup.png" />
<figcaption>
<p><span class="caption-number">Fig. 17 </span><span class="caption-text">The DALL·E 2 signature present in every image it creates via the API <span id="id19">[<a class="reference internal" href="bibliography.html#id20" title="Pamela Mishkin, Lama Ahmad, Miles Brundage, Gretchen Krueger, and Girish Sastry. DALL·E 2 Preview - Risks and Limitations. 2022. URL: https://github.com/openai/dalle-2-preview/blob/main/system-card.md.">MAB+22</a>]</span>.</span><a class="headerlink" href="#signature-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Ausserdem filtert die API Input nach Kriterien wie z.B. Sicherheitsbedenken (sexualisierte oder suggestive Bilder von Kindern, Gewalt, politischer Content und “toxischer Content”).</p>
<p>Allerdings geschieht diese Filtering von Input Text und Input Bild unabhängig voneinander. Demnach könnte man das Model anweisen Inpainting für ein Bild einer Dusche mit dem Text “a woman” zu machen, und dabei potenziell ein Bild einer nackten Frau generieren.</p>
</section>
<section id="diffusion-basics">
<h3><a class="toc-backref" href="#id57">Diffusion Basics</a><a class="headerlink" href="#diffusion-basics" title="Permalink to this headline">#</a></h3>
<p>Diffusion Modelle sind, wie auch GANs, Generative Modelle, sie generieren Daten ähnlich zu den Trainingsdaten mit denen sie trainiert wurden. Beschrieben wurden sie u.a. von <span id="id20">Ho <em>et al.</em> [<a class="reference internal" href="bibliography.html#id18" title="Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. December 2020. URL: http://arxiv.org/abs/2006.11239 (visited on 2022-06-07), arXiv:2006.11239.">HJA20</a>]</span> mit Inspiration aus den “nonequilibrium thermodynamics”.</p>
<blockquote class="epigraph">
<div><p>A diffusion model is trained to undo the steps of a fixed corruption process.</p>
<p class="attribution">—<span id="id21">Ramesh [<a class="reference internal" href="bibliography.html#id25" title="Aditya Ramesh. How DALL·E 2 Works. URL: http://adityaramesh.com/posts/dalle2/dalle2.html (visited on 2022-06-21).">Ram</a>]</span></p>
</div></blockquote>
<p>Jeder Zeitschritt addiert eine kleine Menge Gaussian Noise und verringert somit die Information im Bild. Nach dem letzten Schritt ist das Bild von purer Gaussian Noise nicht mehr unterscheidbar. Das ist der sog. <em>Forward Diffusion Process</em>.</p>
<p>Da ein State bzw. ein Bild immer nur von dem vorherigen abhängig ist, kann der Prozess als Markov Chain modelliert werden (siehe <a class="reference internal" href="#markov-fig"><span class="std std-numref">Fig. 18</span></a>).</p>
<p>Im <em>Reverse Diffusion Process</em> wird das Model wird nun darauf trainiert, den Prozess Schritt für Schritt rückgängig zu machen. Dabei lernt es Informationen wiederherzustellen, die jedem Schritt existiert haben könnten.</p>
<figure class="align-default" id="markov-fig">
<img alt="_images/diffusion_markov.png" src="_images/diffusion_markov.png" />
<figcaption>
<p><span class="caption-number">Fig. 18 </span><span class="caption-text">Forward <span class="math notranslate nohighlight">\(q(x_t|x_{t-1})\)</span> and Backwards <span class="math notranslate nohighlight">\(p_{\theta}(x_{t-1}|x_t)\)</span> Diffusion Process as a Markov Chain. <span id="id22">[<a class="reference internal" href="bibliography.html#id18" title="Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. December 2020. URL: http://arxiv.org/abs/2006.11239 (visited on 2022-06-07), arXiv:2006.11239.">HJA20</a>]</span></span><a class="headerlink" href="#markov-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Gibt man nun Rauschen auf das gelernte Model und wendet es mehrfach darauf an (<span class="math notranslate nohighlight">\(\approx 1000\)</span> Schritte sind üblich), dann wird das Bild immer realistischer bis es komplett rauschfrei ist und aus der Verteilung der Trainingsdaten stammt.</p>
<p>Ein grosser Vorteil der Diffusion Modelle gegenüber GANs ist, dass kein Adversarial Training erforderlich ist. Ausserdem ist die Implementierung sehr flexibel, die einzige Anforderung an die darunterliegende Architektur ist, dass Input und Output gleich gross sein müssen. In der Praxis werden häufig UNets verwendet.</p>
<p>Nachteil der Diffusion Modelle gegenüber GANs sind jedoch die längeren Inferenzzeiten.</p>
<figure class="align-default" id="id44">
<img alt="_images/denoising_different_steps.png" src="_images/denoising_different_steps.png" />
<figcaption>
<p><span class="caption-number">Fig. 19 </span><span class="caption-text">DDPM applied on CelebA-HQ, showing the Reverse Process starting at different timesteps <span id="id23">[<a class="reference internal" href="bibliography.html#id18" title="Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. December 2020. URL: http://arxiv.org/abs/2006.11239 (visited on 2022-06-07), arXiv:2006.11239.">HJA20</a>]</span></span><a class="headerlink" href="#id44" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="architecture">
<h3><a class="toc-backref" href="#id58">Architecture</a><a class="headerlink" href="#architecture" title="Permalink to this headline">#</a></h3>
<p>Wie bereits beschrieben besteht DALL·E 2 aus 2 Komponenten:</p>
<ul class="simple">
<li><p>Der <em>Prior</em> <span class="math notranslate nohighlight">\(P(z_i|y)\)</span> erzeugt CLIP Image Embeddings <span class="math notranslate nohighlight">\(z_i\)</span> unter gegebener Caption <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
<li><p>Der <em>Decoder</em> <span class="math notranslate nohighlight">\(P(x|z_i,y)\)</span> erzeugt Bilder <span class="math notranslate nohighlight">\(x\)</span> unter gegebenen Image Embeddings <span class="math notranslate nohighlight">\(z_i\)</span> (und optional auch der Caption <span class="math notranslate nohighlight">\(y\)</span>, bleibt aber hier ungenutzt).</p></li>
</ul>
<p>Beide zusammen ergeben ein Generative Model <span class="math notranslate nohighlight">\(P(x|y)\)</span> für die Bilder <span class="math notranslate nohighlight">\(x\)</span> mit gegebenen Captions <span class="math notranslate nohighlight">\(y\)</span>. <span class="math notranslate nohighlight">\(P(x|y)\)</span> kann geschrieben werden als</p>
<div class="math notranslate nohighlight">
\[P( x | y ) = P(x, z_i | y),\]</div>
<p>weil <span class="math notranslate nohighlight">\(z_i\)</span> eine deterministische Funktion von <span class="math notranslate nohighlight">\(x\)</span> ist. Weiter kann man mit der Kettenregel umformen und erhält</p>
<div class="math notranslate nohighlight">
\[P( x | y ) =  P(x, z_i | y) = P(x | z_i, y)P(z_i|y)
\]</div>
<p>Man kann also aus der echten Verteilung <span class="math notranslate nohighlight">\(P(x|y)\)</span> samplen, indem man erst <span class="math notranslate nohighlight">\(z_i\)</span> mit dem Prior sampled und dann <span class="math notranslate nohighlight">\(x\)</span> mit dem Decoder.</p>
<p>Was bisher nicht erwähnt wurde sind die 2 Upsampling Diffusion Models:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(64 \times 64 \rightarrow 256 \times 256\)</span> und</p></li>
<li><p><span class="math notranslate nohighlight">\(256 \times 256 \rightarrow 1024 \times 1024\)</span></p></li>
</ul>
<figure class="align-default" id="hyperparams-fig">
<img alt="_images/hyperparameters.png" src="_images/hyperparameters.png" />
<figcaption>
<p><span class="caption-number">Fig. 20 </span><span class="caption-text">Hyperparameters <span id="id24">[<a class="reference internal" href="bibliography.html#id24" title="Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical Text-Conditional Image Generation with CLIP Latents. April 2022. URL: http://arxiv.org/abs/2204.06125 (visited on 2022-06-09), arXiv:2204.06125, doi:10.48550/arXiv.2204.06125.">RDN+22</a>]</span></span><a class="headerlink" href="#hyperparams-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<section id="prior">
<h4>Prior<a class="headerlink" href="#prior" title="Permalink to this headline">#</a></h4>
<p>Der Prior generiert aus dem Labeln <span class="math notranslate nohighlight">\(y\)</span> ein CLIP Image Embedding <span class="math notranslate nohighlight">\(z_i\)</span>.
Hierfür haben die Autoren zwei verschiedene Model Klassen getestet, einen <em>Autoregressive (AR)</em> Prior und einen <em>Diffusion</em> Prior. Letzterer wurde als effizienter und qualitativ hochwertiger befunden und im Folgenden genauer betrachtet.</p>
<section id="diffusion-prior">
<h5>Diffusion Prior<a class="headerlink" href="#diffusion-prior" title="Permalink to this headline">#</a></h5>
<p>Wie bereits oben erwähnt ist eine Anforderung an Diffusion Modellen, dass Input und Output die gleiche Grösse haben, bei DALL·E 2 wird hier ein <em>Decoder-Only Transformer</em> mit <em>Casual Attention Mask</em> verwendet.</p>
<p>Um die Qualität zu verbessern, werden bei jedem Sampling je 2 <span class="math notranslate nohighlight">\(z_i\)</span> Samples generiert und das Sample ausgewählt das ein höheres <span class="math notranslate nohighlight">\(z_i \cdot z_t\)</span> aufweist. Ein höheres Skalarprodukt der beiden Embeddings bedeutet dass die Caption das Bild besser beschreibt (vgl. Training von <a class="reference internal" href="#clip"><span class="std std-ref">CLIP</span></a>).</p>
<div class="dropdown admonition">
<p class="admonition-title">Transformer Sequence</p>
<p>Die Sequenz auf welcher der Transformer agiert besteht aus:</p>
<ul class="simple">
<li><p>Tokenization des Text</p></li>
<li><p>CLIP Text Embedding <span class="math notranslate nohighlight">\(z_t\)</span> der Tokens</p></li>
<li><p>Embedding des sktuellen Diffusion Timesteps</p></li>
<li><p>Noised CLIP Image Embedding zum aktuellen Timestep</p></li>
<li><p>“Final Embedding” welches verwendet wird um das Denoised CLIP Image Embedding <span class="math notranslate nohighlight">\(z_i\)</span> vorherzusagen</p></li>
</ul>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Loss Function</p>
<p>Der verwendete Loss ist eine Vereinfachung von der von <span id="id25">Ho <em>et al.</em> [<a class="reference internal" href="bibliography.html#id18" title="Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. December 2020. URL: http://arxiv.org/abs/2006.11239 (visited on 2022-06-07), arXiv:2006.11239.">HJA20</a>]</span> verwendeten Loss Function bei DDPM, es wird lediglich der Mean Squared Error zwischen echtem <span class="math notranslate nohighlight">\(z_i\)</span> und der Vorhersage berechnet:</p>
<div class="math notranslate nohighlight">
\[L_{prior} = \mathbb{E}_{t \sim [1,T], z_i^{(t)} \sim q_t} [\|  f_{\theta}(z_i^{(t)}, t, y) - z_i \|^2]\]</div>
</div>
</section>
</section>
<section id="decoder">
<h4>Decoder<a class="headerlink" href="#decoder" title="Permalink to this headline">#</a></h4>
<p>Der Decoder der in DALL·E 2 Verwendung findet ist eine leichte Modifikation des Diffusions Models <a class="reference internal" href="#glide"><span class="std std-ref">GLIDE</span></a> von <span id="id26">Nichol <em>et al.</em> [<a class="reference internal" href="bibliography.html#id21" title="Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. March 2022. URL: http://arxiv.org/abs/2112.10741 (visited on 2022-06-07), arXiv:2112.10741.">NDR+22</a>]</span>.
Zur Erinnerung: GLIDE generiert Bilder direkt aus Text Embeddings, ohne Umweg über CLIP Embeddings.
Um diese Embeddings nun bei gleichbleibender Architektur in den Decoder einzubringen, werden die sie in das existierende Timestep Embedding projiziert. Ausserdem werden sie auch noch in 4 extra Token projiziert die an die Text Tokens des GLIDE Text Encoders angehängt werden.</p>
<p>Die Autoren entschieden sich für das Beibehalten des “<em>text-conditioning pathway</em>” des GLIDE Decoders unter der Annahme, dass das Diffusion Model dadurch Aspekte von Natural Language lernen könnte, die CLIP nicht erfasst, wie beispielsweise Variable Binding. Letztendlich wird er in DALL·E 2 aber nicht verwendet, weil er zu keinen merklichen Verbesserungen führte.</p>
<p>Durch sog. <em>Classifier-free Guidance</em> wurde die Sampling Qualität weiter verbessert, vorgestellt wurde diese Technik von <span id="id27">Ho and Salimans [<a class="reference internal" href="bibliography.html#id17" title="Jonathan Ho and Tim Salimans. Classifier-Free Diffusion Guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications. November 2021. URL: https://openreview.net/forum?id=qw8AKxfYbI (visited on 2022-06-26).">HS21</a>]</span>.</p>
</section>
<section id="upsampler">
<h4>Upsampler<a class="headerlink" href="#upsampler" title="Permalink to this headline">#</a></h4>
<p>Die Diffusion Upsampler (<span class="math notranslate nohighlight">\(64^2 \rightarrow 256^2\)</span> und <span class="math notranslate nohighlight">\(256^2 \rightarrow 1024^2\)</span>) folgen der <em>Unconditional ADMNet</em> Architektur aus <a class="reference external" href="http://arxiv.org/abs/2105.05233">“Diffusion Models Beat GANs on Image Synthesis”</a> von <span id="id28">Dhariwal and Nichol [<a class="reference internal" href="bibliography.html#id11" title="Prafulla Dhariwal and Alex Nichol. Diffusion Models Beat GANs on Image Synthesis. June 2021. URL: http://arxiv.org/abs/2105.05233 (visited on 2022-06-07), arXiv:2105.05233.">DN21</a>]</span>.</p>
<p>Trainiert wurden die ADMNets auf leicht korrumpierten Bildern um die Qualität des Upsamplings zu verbessern. In Experimenten hat sich ausserdem gezeigt, dass eine Konditionierung auf die Bild Captions keinen sichtbaren Verbesserungen mit sich bringt.</p>
</section>
</section>
<section id="training-dataset">
<h3><a class="toc-backref" href="#id59">Training Dataset</a><a class="headerlink" href="#training-dataset" title="Permalink to this headline">#</a></h3>
<p>Ähnlich wie schon bei GLIDE wurden die Trainingsdaten auf denen das Model trainiert wurde, gefiltert, um “explicit content” gering zu halten. Diesmal jedoch weniger aggressiv: Bilder und Captions die “grafische sexuelle sowie gewalttätige Inhalte” haben wurden entfernt, ebenso wie “Hate Symbols”.</p>
<p>Der Trainingsdatensatz besteht aus 2 Teilen:</p>
<ul class="simple">
<li><p>dem CLIP Datensatz <span class="math notranslate nohighlight">\(\approx 400M\)</span> <span id="id29">[<a class="reference internal" href="bibliography.html#id23" title="Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. February 2021. URL: http://arxiv.org/abs/2103.00020 (visited on 2022-06-07), arXiv:2103.00020.">RKH+21</a>]</span></p></li>
<li><p>und dem DALL·E Datensatz <span class="math notranslate nohighlight">\(\approx 250M\)</span> <span id="id30">[<a class="reference internal" href="bibliography.html#id26" title="Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-Shot Text-to-Image Generation. February 2021. URL: http://arxiv.org/abs/2102.12092 (visited on 2022-06-12), arXiv:2102.12092, doi:10.48550/arXiv.2102.12092.">RPG+21</a>]</span></p></li>
</ul>
<p>Die CLIP Encoder wurden auf beiden trainiert und dann eingefroren. Prior, Decoder und Upsampler hingegen nur auf letzterem.</p>
</section>
<section id="limitations">
<h3><a class="toc-backref" href="#id60">Limitations</a><a class="headerlink" href="#limitations" title="Permalink to this headline">#</a></h3>
<p>Trotz des grossen Fortschritts in Fidelity und Diversity der Samples die DALL·E 2 generieren kann, hat es auch einige Limitationen. Ganz besonders ist es schlechter als GLIDE im Bereich Variable Binding (siehe <a class="reference internal" href="#dalle2-variable-fig"><span class="std std-numref">Fig. 21</span></a>). Die Hypothese der Autoren ist dass CLIP selbst keine expliziten Attributs Zuweisung modellieren kann.
Aber auch detailreichere Szenen oder zusammenängenden Text (siehe <a class="reference internal" href="#dalle2-text-fig"><span class="std std-numref">Fig. 22</span></a>) schafft unCLIP nicht korrekt darzustellen.</p>
<figure class="align-default" id="dalle2-variable-fig">
<img alt="_images/dalle2_figure15.png" src="_images/dalle2_figure15.png" />
<figcaption>
<p><span class="caption-number">Fig. 21 </span><span class="caption-text">unCLIP confusing variable assignments <span id="id31">[<a class="reference internal" href="bibliography.html#id24" title="Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical Text-Conditional Image Generation with CLIP Latents. April 2022. URL: http://arxiv.org/abs/2204.06125 (visited on 2022-06-09), arXiv:2204.06125, doi:10.48550/arXiv.2204.06125.">RDN+22</a>]</span>.</span><a class="headerlink" href="#dalle2-variable-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="dalle2-text-fig">
<img alt="_images/dalle2_figure16.png" src="_images/dalle2_figure16.png" />
<figcaption>
<p><span class="caption-number">Fig. 22 </span><span class="caption-text">unCLIP prompted with: “A sign that says deep learning.” <span id="id32">[<a class="reference internal" href="bibliography.html#id24" title="Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical Text-Conditional Image Generation with CLIP Latents. April 2022. URL: http://arxiv.org/abs/2204.06125 (visited on 2022-06-09), arXiv:2204.06125, doi:10.48550/arXiv.2204.06125.">RDN+22</a>]</span>.</span><a class="headerlink" href="#dalle2-text-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>In <a class="reference internal" href="#dalle2-compared-fig"><span class="std std-numref">Fig. 23</span></a> sieht man eine groessere Gegenueberstellung der bisherigen Text To Image Modelle.</p>
<figure class="align-default" id="dalle2-compared-fig">
<img alt="_images/dalle2_figure12.png" src="_images/dalle2_figure12.png" />
<figcaption>
<p><span class="caption-number">Fig. 23 </span><span class="caption-text">DALL·E vs GLIDE vs unCLIP on COCO <span id="id33">[<a class="reference internal" href="bibliography.html#id24" title="Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical Text-Conditional Image Generation with CLIP Latents. April 2022. URL: http://arxiv.org/abs/2204.06125 (visited on 2022-06-09), arXiv:2204.06125, doi:10.48550/arXiv.2204.06125.">RDN+22</a>]</span></span><a class="headerlink" href="#dalle2-compared-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="further-work">
<h2><a class="toc-backref" href="#id61">Further Work</a><a class="headerlink" href="#further-work" title="Permalink to this headline">#</a></h2>
<section id="google-imagen">
<h3><a class="toc-backref" href="#id62">Google Imagen</a><a class="headerlink" href="#google-imagen" title="Permalink to this headline">#</a></h3>
<figure class="align-default" id="id45">
<img alt="_images/imagen_header.png" src="_images/imagen_header.png" />
<figcaption>
<p><span class="caption-number">Fig. 24 </span><span class="caption-text">A cherrypicked collection of Imagen generations <span id="id34">[<a class="reference internal" href="bibliography.html#id28" title="Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. May 2022. URL: http://arxiv.org/abs/2205.11487 (visited on 2022-06-12), arXiv:2205.11487, doi:10.48550/arXiv.2205.11487.">SCS+22</a>]</span>.</span><a class="headerlink" href="#id45" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Googles Imagen wurde am 13. Mai 2022 in dem Paper <a class="reference external" href="http://arxiv.org/abs/2205.11487">Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</a> <span id="id35">[<a class="reference internal" href="bibliography.html#id28" title="Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. May 2022. URL: http://arxiv.org/abs/2205.11487 (visited on 2022-06-12), arXiv:2205.11487, doi:10.48550/arXiv.2205.11487.">SCS+22</a>]</span> vorgestellt und hat DALL·E 2 als State of The Art Model abgeloest.</p>
<figure class="align-default" id="id46">
<a class="reference internal image-reference" href="_images/imagen_overview.png"><img alt="_images/imagen_overview.png" src="_images/imagen_overview.png" style="height: 512px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 25 </span><span class="caption-text">Overview of the Imagen architecture <span id="id36">[<a class="reference internal" href="bibliography.html#id28" title="Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. May 2022. URL: http://arxiv.org/abs/2205.11487 (visited on 2022-06-12), arXiv:2205.11487, doi:10.48550/arXiv.2205.11487.">SCS+22</a>]</span>.</span><a class="headerlink" href="#id46" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Imagen verzichtet hier also auf das “Layer of Indirection” über die CLIP Image Embeddings, verwendet wird stattdessen der mächtige Text Encoder <code class="docutils literal notranslate"><span class="pre">T5-XXL</span></code>, womit Imagen GLIDE ähnelt. Des weiteren wurden hier Text-Conditional Super-Resolution Modelle angewandt statt Unconditional wie in DALL·E 2.</p>
<figure class="align-default" id="id47">
<img alt="_images/imagen_dalle2_glide_backpack.png" src="_images/imagen_dalle2_glide_backpack.png" />
<figcaption>
<p><span class="caption-number">Fig. 26 </span><span class="caption-text">Imagen vs DALL·E 2 vs GLIDE: “A black apple and a green backpack.”.<br />
Adapted from <span id="id37">[<a class="reference internal" href="bibliography.html#id28" title="Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. May 2022. URL: http://arxiv.org/abs/2205.11487 (visited on 2022-06-12), arXiv:2205.11487, doi:10.48550/arXiv.2205.11487.">SCS+22</a>]</span>.</span><a class="headerlink" href="#id47" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id48">
<img alt="_images/imagen_dalle2_glide_text.png" src="_images/imagen_dalle2_glide_text.png" />
<figcaption>
<p><span class="caption-number">Fig. 27 </span><span class="caption-text">Imagen vs DALL·E 2 vs GLIDE: “A storefront with Text to Image written on it.”.<br />
Adapted from <span id="id38">[<a class="reference internal" href="bibliography.html#id28" title="Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. May 2022. URL: http://arxiv.org/abs/2205.11487 (visited on 2022-06-12), arXiv:2205.11487, doi:10.48550/arXiv.2205.11487.">SCS+22</a>]</span>.</span><a class="headerlink" href="#id48" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="questions-and-discussion">
<h2><a class="toc-backref" href="#id63">Questions and Discussion</a><a class="headerlink" href="#questions-and-discussion" title="Permalink to this headline">#</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Was wuerdet ihr mit DALL·E 2 anfangen?</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="intro.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Introduction and Overview</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="bonus.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Further Reading</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Jason Schuehlein (js450)<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>